Activating virtual environment:  SNPNet
/faststorage/project/NLPPred/snp-compression/SNPNet/bin/python
Training SNPNet
wandb: Currently logged in as: kenevoldsen (use `wandb login --relogin` to force relogin)
wandb: Tracking run with wandb version 0.12.10
wandb: Syncing run trim-thunder-51
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kenevoldsen/snp-compression
wandb: üöÄ View run at https://wandb.ai/kenevoldsen/snp-compression/runs/3rqmjfm4
wandb: Run data is saved locally in /home/kce/NLPPred/snp-compression/models/wandb/run-20220221_183545-3rqmjfm4
wandb: Run `wandb offline` to turn off syncing.
Using 16bit native Automatic Mixed Precision (AMP)
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Set SLURM handle signals.

FIT Profiler Report

Action                             	|  Mean duration (s)	|Num calls      	|  Total time (s) 	|  Percentage %   	|
--------------------------------------------------------------------------------------------------------------------------------------
Total                              	|  -              	|_              	|  10.798         	|  100 %          	|
--------------------------------------------------------------------------------------------------------------------------------------
get_sanity_check_batch             	|  0.60119        	|3              	|  1.8036         	|  16.703         	|
fetch_next_sanity_check_batch      	|  0.60116        	|3              	|  1.8035         	|  16.702         	|
evaluation_step_and_end            	|  0.59224        	|2              	|  1.1845         	|  10.97          	|
validation_step                    	|  0.59212        	|2              	|  1.1842         	|  10.967         	|
run_training_epoch                 	|  0.26406        	|1              	|  0.26406        	|  2.4455         	|
get_train_batch                    	|  0.048994       	|1              	|  0.048994       	|  0.45374        	|
fetch_next_train_batch             	|  0.048921       	|1              	|  0.048921       	|  0.45306        	|
evaluation_batch_to_device         	|  0.0043963      	|2              	|  0.0087926      	|  0.08143        	|
on_train_start                     	|  0.0026162      	|1              	|  0.0026162      	|  0.024229       	|
configure_optimizers               	|  0.0015666      	|1              	|  0.0015666      	|  0.014508       	|
on_validation_model_eval           	|  0.0011386      	|1              	|  0.0011386      	|  0.010544       	|
on_validation_batch_start          	|  4.1543e-05     	|2              	|  8.3085e-05     	|  0.00076946     	|
on_configure_sharded_model         	|  4.462e-05      	|1              	|  4.462e-05      	|  0.00041323     	|
on_pretrain_routine_start          	|  3.8682e-05     	|1              	|  3.8682e-05     	|  0.00035824     	|
on_validation_batch_end            	|  1.8149e-05     	|2              	|  3.6297e-05     	|  0.00033616     	|
setup                              	|  3.5776e-05     	|1              	|  3.5776e-05     	|  0.00033133     	|
on_train_epoch_end                 	|  3.4885e-05     	|1              	|  3.4885e-05     	|  0.00032308     	|
on_sanity_check_start              	|  3.2023e-05     	|1              	|  3.2023e-05     	|  0.00029657     	|
on_validation_start                	|  2.8504e-05     	|1              	|  2.8504e-05     	|  0.00026398     	|
on_epoch_start                     	|  1.3508e-05     	|2              	|  2.7016e-05     	|  0.0002502      	|
teardown                           	|  2.4205e-05     	|1              	|  2.4205e-05     	|  0.00022417     	|
on_before_accelerator_backend_setup	|  2.3931e-05     	|1              	|  2.3931e-05     	|  0.00022163     	|
on_validation_end                  	|  2.249e-05      	|1              	|  2.249e-05      	|  0.00020828     	|
configure_callbacks                	|  2.1828e-05     	|1              	|  2.1828e-05     	|  0.00020216     	|
configure_sharded_model            	|  1.9096e-05     	|1              	|  1.9096e-05     	|  0.00017685     	|
validation_step_end                	|  9.3784e-06     	|2              	|  1.8757e-05     	|  0.00017371     	|
on_epoch_end                       	|  9.2825e-06     	|2              	|  1.8565e-05     	|  0.00017193     	|
on_validation_epoch_end            	|  1.7406e-05     	|1              	|  1.7406e-05     	|  0.0001612      	|
on_pretrain_routine_end            	|  1.5587e-05     	|1              	|  1.5587e-05     	|  0.00014435     	|
on_sanity_check_end                	|  1.5216e-05     	|1              	|  1.5216e-05     	|  0.00014092     	|
on_train_end                       	|  1.4599e-05     	|1              	|  1.4599e-05     	|  0.00013521     	|
on_validation_epoch_start          	|  1.3815e-05     	|1              	|  1.3815e-05     	|  0.00012795     	|
on_val_dataloader                  	|  1.0237e-05     	|1              	|  1.0237e-05     	|  9.4807e-05     	|
val_dataloader                     	|  9.4306e-06     	|1              	|  9.4306e-06     	|  8.7338e-05     	|
prepare_data                       	|  8.0522e-06     	|1              	|  8.0522e-06     	|  7.4573e-05     	|
on_train_epoch_start               	|  6.0797e-06     	|1              	|  6.0797e-06     	|  5.6305e-05     	|
on_train_dataloader                	|  5.0813e-06     	|1              	|  5.0813e-06     	|  4.7059e-05     	|
train_dataloader                   	|  3.3639e-06     	|1              	|  3.3639e-06     	|  3.1154e-05     	|

LR finder stopped early after 0 steps due to diverging loss.
Restoring states from the checkpoint path at /home/kce/NLPPred/snp-compression/models/lr_find_temp_model_72240e49-561b-45e9-9d64-a1bfb5d68d92.ckpt
Failed to compute suggesting for `lr`. There might not be enough points.
Traceback (most recent call last):
  File "/faststorage/project/NLPPred/snp-compression/SNPNet/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 176, in suggestion
    min_grad = np.gradient(loss).argmin()
  File "<__array_function__ internals>", line 180, in gradient
  File "/faststorage/project/NLPPred/snp-compression/SNPNet/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1195, in gradient
    raise ValueError(
ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
Failed to compute suggesting for `lr`. There might not be enough points.
Traceback (most recent call last):
  File "/faststorage/project/NLPPred/snp-compression/SNPNet/lib/python3.8/site-packages/pytorch_lightning/tuner/lr_finder.py", line 176, in suggestion
    min_grad = np.gradient(loss).argmin()
  File "<__array_function__ internals>", line 180, in gradient
  File "/faststorage/project/NLPPred/snp-compression/SNPNet/lib/python3.8/site-packages/numpy/lib/function_base.py", line 1195, in gradient
    raise ValueError(
ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Set SLURM handle signals.

  | Name     | Type                 | Params
--------------------------------------------------
0 | model    | DenoisingAutoencoder | 574 K 
1 | loss     | CrossEntropyLoss     | 0     
2 | accuracy | Accuracy             | 0     
3 | f1       | F1Score              | 0     
4 | conf_mat | ConfusionMatrix      | 0     
--------------------------------------------------
574 K     Trainable params
0         Non-trainable params
574 K     Total params
1.148     Total estimated model params size (MB)
Validation sanity check: 0it [00:00, ?it/s]Validation sanity check:   0% 0/2 [00:00<?, ?it/s]Validation sanity check:  50% 1/2 [00:02<00:02,  2.65s/it]Validation sanity check: 100% 2/2 [00:03<00:00,  1.39s/it]                                                          /faststorage/project/NLPPred/snp-compression/SNPNet/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning:

There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.

Training: 0it [00:00, ?it/s]Training: 0it [00:00, ?it/s]Epoch 0: : 0it [00:00, ?it/s]Epoch 0: : 1it [00:00,  2.80it/s]Epoch 0: : 1it [00:00,  2.80it/s, loss=nan, v_num=jfm4]Epoch 0: : 1it [00:00,  2.79it/s, loss=nan, v_num=jfm4]FIT Profiler Report

Action                             	|  Mean duration (s)	|Num calls      	|  Total time (s) 	|  Percentage %   	|
--------------------------------------------------------------------------------------------------------------------------------------
Total                              	|  -              	|_              	|  15.233         	|  100 %          	|
--------------------------------------------------------------------------------------------------------------------------------------
get_sanity_check_batch             	|  0.59308        	|6              	|  3.5585         	|  23.361         	|
fetch_next_sanity_check_batch      	|  0.59305        	|6              	|  3.5583         	|  23.36          	|
evaluation_step_and_end            	|  0.56602        	|4              	|  2.2641         	|  14.863         	|
validation_step                    	|  0.56586        	|4              	|  2.2634         	|  14.859         	|
run_training_epoch                 	|  0.31101        	|2              	|  0.62202        	|  4.0835         	|
get_train_batch                    	|  0.084904       	|2              	|  0.16981        	|  1.1148         	|
fetch_next_train_batch             	|  0.084826       	|2              	|  0.16965        	|  1.1137         	|
evaluation_batch_to_device         	|  0.004123       	|4              	|  0.016492       	|  0.10827        	|
on_pretrain_routine_start          	|  0.0066861      	|2              	|  0.013372       	|  0.087787       	|
on_train_start                     	|  0.0052303      	|2              	|  0.010461       	|  0.068673       	|
configure_optimizers               	|  0.0014195      	|2              	|  0.002839       	|  0.018638       	|
on_validation_model_eval           	|  0.0010664      	|2              	|  0.0021327      	|  0.014001       	|
on_validation_batch_end            	|  0.000424       	|4              	|  0.001696       	|  0.011134       	|
on_sanity_check_start              	|  0.00060055     	|2              	|  0.0012011      	|  0.0078851      	|
on_validation_end                  	|  0.00046323     	|2              	|  0.00092646     	|  0.0060821      	|
on_train_epoch_end                 	|  0.00042391     	|2              	|  0.00084783     	|  0.0055659      	|
on_train_epoch_start               	|  0.00022258     	|2              	|  0.00044516     	|  0.0029224      	|
on_validation_start                	|  0.00014999     	|2              	|  0.00029999     	|  0.0019694      	|
on_train_end                       	|  0.00010723     	|2              	|  0.00021446     	|  0.0014079      	|
on_validation_batch_start          	|  4.8731e-05     	|4              	|  0.00019492     	|  0.0012796      	|
on_validation_epoch_end            	|  5.152e-05      	|2              	|  0.00010304     	|  0.00067644     	|
on_configure_sharded_model         	|  4.6928e-05     	|2              	|  9.3857e-05     	|  0.00061616     	|
on_epoch_start                     	|  2.3178e-05     	|4              	|  9.2711e-05     	|  0.00060864     	|
on_sanity_check_end                	|  3.9212e-05     	|2              	|  7.8425e-05     	|  0.00051485     	|
on_before_accelerator_backend_setup	|  3.8305e-05     	|2              	|  7.6611e-05     	|  0.00050294     	|
setup                              	|  3.6611e-05     	|2              	|  7.3222e-05     	|  0.00048069     	|
on_epoch_end                       	|  1.7242e-05     	|4              	|  6.8966e-05     	|  0.00045275     	|
teardown                           	|  3.1311e-05     	|2              	|  6.2622e-05     	|  0.00041111     	|
validation_step_end                	|  1.3588e-05     	|4              	|  5.4354e-05     	|  0.00035683     	|
on_pretrain_routine_end            	|  2.3839e-05     	|2              	|  4.7678e-05     	|  0.000313       	|
on_validation_epoch_start          	|  2.0271e-05     	|2              	|  4.0542e-05     	|  0.00026615     	|
on_fit_end                         	|  4.0231e-05     	|1              	|  4.0231e-05     	|  0.00026411     	|
configure_callbacks                	|  2.0079e-05     	|2              	|  4.0159e-05     	|  0.00026364     	|
on_fit_start                       	|  3.7383e-05     	|1              	|  3.7383e-05     	|  0.00024542     	|
configure_sharded_model            	|  1.7739e-05     	|2              	|  3.5478e-05     	|  0.00023291     	|
prepare_data                       	|  1.0973e-05     	|2              	|  2.1946e-05     	|  0.00014407     	|
on_val_dataloader                  	|  1.015e-05      	|2              	|  2.0301e-05     	|  0.00013327     	|
on_train_dataloader                	|  8.4303e-06     	|2              	|  1.6861e-05     	|  0.00011069     	|
val_dataloader                     	|  9.4306e-06     	|1              	|  9.4306e-06     	|  6.191e-05      	|
train_dataloader                   	|  3.3639e-06     	|1              	|  3.3639e-06     	|  2.2084e-05     	|


wandb: Waiting for W&B process to finish, PID 38822... (success).
wandb: - 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: \ 0.05MB of 0.05MB uploaded (0.00MB deduped)wandb: | 0.05MB of 0.07MB uploaded (0.00MB deduped)wandb: / 0.05MB of 0.07MB uploaded (0.00MB deduped)wandb: - 0.05MB of 0.07MB uploaded (0.00MB deduped)wandb: \ 0.05MB of 0.07MB uploaded (0.00MB deduped)wandb: | 0.07MB of 0.07MB uploaded (0.00MB deduped)wandb: / 0.07MB of 0.07MB uploaded (0.00MB deduped)wandb: - 0.07MB of 0.07MB uploaded (0.00MB deduped)wandb: \ 0.07MB of 0.07MB uploaded (0.00MB deduped)wandb: | 0.07MB of 0.07MB uploaded (0.00MB deduped)wandb: / 0.07MB of 0.07MB uploaded (0.00MB deduped)wandb: - 0.07MB of 0.07MB uploaded (0.00MB deduped)wandb: \ 0.07MB of 0.07MB uploaded (0.00MB deduped)wandb: | 0.07MB of 0.07MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:   global_step ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:   global_step 0
wandb: 
wandb: Synced 5 W&B file(s), 3 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced trim-thunder-51: https://wandb.ai/kenevoldsen/snp-compression/runs/3rqmjfm4
wandb: Find logs at: /home/kce/NLPPred/snp-compression/models/wandb/run-20220221_183545-3rqmjfm4/logs/debug.log
wandb: 

