wandb: Currently logged in as: kenevoldsen (use `wandb login --relogin` to force relogin)
wandb: Tracking run with wandb version 0.12.1
wandb: Syncing run quiet-river-27
wandb:  View project at https://wandb.ai/kenevoldsen/snp-compression-src_models
wandb:  View run at https://wandb.ai/kenevoldsen/snp-compression-src_models/runs/2rgw393s
wandb: Run data is saved locally in /home/kce/github/snp-compression/wandb/run-20210827_122644-2rgw393s
wandb: Run `wandb offline` to turn off syncing.
GPU available: True, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
/home/kce/miniconda3/envs/snp-compression/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1292: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.
  rank_zero_warn(
Set SLURM handle signals.

  | Name     | Type                 | Params
--------------------------------------------------
0 | model    | DenoisingAutoencoder | 266 K 
1 | loss     | CrossEntropyLoss     | 0     
2 | accuracy | Accuracy             | 0     
3 | conf_mat | ConfusionMatrix      | 0     
4 | f1       | F1                   | 0     
--------------------------------------------------
266 K     Trainable params
0         Non-trainable params
266 K     Total params
1.065     Total estimated model params size (MB)

Validation sanity check: 0it [00:00, ?it/s]Validation sanity check:   0% 0/2 [00:00<?, ?it/s]Validation sanity check:  50% 1/2 [00:00<00:00,  1.60it/s]Validation sanity check: 100% 2/2 [00:00<00:00,  2.23it/s]                                                          Training: -1it [00:00, ?it/s]Training:   0% 0/1930 [00:00<00:00, 51150.05it/s]Epoch 0:   0% 0/1930 [00:00<00:00, 9098.27it/s]  Epoch 0:   0% 1/1930 [00:00<14:56,  2.15it/s]  Epoch 0:   0% 1/1930 [00:00<14:57,  2.15it/s, loss=1.39, v_num=393s]Epoch 0:   0% 2/1930 [00:01<16:54,  1.90it/s, loss=1.39, v_num=393s]Epoch 0:   0% 2/1930 [00:01<16:54,  1.90it/s, loss=1.39, v_num=393s]Epoch 0:   0% 3/1930 [00:02<17:37,  1.82it/s, loss=1.39, v_num=393s]Epoch 0:   0% 3/1930 [00:02<17:38,  1.82it/s, loss=1.39, v_num=393s]Epoch 0:   0% 4/1930 [00:02<18:02,  1.78it/s, loss=1.39, v_num=393s]Epoch 0:   0% 4/1930 [00:02<18:03,  1.78it/s, loss=1.39, v_num=393s]Epoch 0:   0% 5/1930 [00:03<18:23,  1.74it/s, loss=1.39, v_num=393s]Epoch 0:   0% 5/1930 [00:03<18:23,  1.74it/s, loss=1.39, v_num=393s]Epoch 0:   0% 6/1930 [00:04<18:45,  1.71it/s, loss=1.39, v_num=393s]Epoch 0:   0% 6/1930 [00:04<18:45,  1.71it/s, loss=1.39, v_num=393s]Epoch 0:   0% 7/1930 [00:04<18:56,  1.69it/s, loss=1.39, v_num=393s]Epoch 0:   0% 7/1930 [00:04<18:57,  1.69it/s, loss=1.39, v_num=393s]Epoch 0:   0% 8/1930 [00:05<19:05,  1.68it/s, loss=1.39, v_num=393s]Epoch 0:   0% 8/1930 [00:05<19:05,  1.68it/s, loss=1.39, v_num=393s]Epoch 0:   0% 9/1930 [00:06<19:15,  1.66it/s, loss=1.39, v_num=393s]Epoch 0:   0% 9/1930 [00:06<19:15,  1.66it/s, loss=1.39, v_num=393s]Epoch 0:   1% 10/1930 [00:06<19:20,  1.65it/s, loss=1.39, v_num=393s]Epoch 0:   1% 10/1930 [00:06<19:20,  1.65it/s, loss=1.39, v_num=393s]Epoch 0:   1% 11/1930 [00:07<19:24,  1.65it/s, loss=1.39, v_num=393s]Epoch 0:   1% 11/1930 [00:07<19:24,  1.65it/s, loss=1.39, v_num=393s]Epoch 0:   1% 12/1930 [00:07<19:29,  1.64it/s, loss=1.39, v_num=393s]Epoch 0:   1% 12/1930 [00:07<19:29,  1.64it/s, loss=1.39, v_num=393s]Epoch 0:   1% 13/1930 [00:08<19:32,  1.63it/s, loss=1.39, v_num=393s]Epoch 0:   1% 13/1930 [00:08<19:32,  1.63it/s, loss=1.39, v_num=393s]Epoch 0:   1% 14/1930 [00:09<19:38,  1.63it/s, loss=1.39, v_num=393s]Epoch 0:   1% 14/1930 [00:09<19:38,  1.63it/s, loss=1.39, v_num=393s]Epoch 0:   1% 15/1930 [00:09<19:47,  1.61it/s, loss=1.39, v_num=393s]Epoch 0:   1% 15/1930 [00:09<19:47,  1.61it/s, loss=1.39, v_num=393s]Epoch 0:   1% 16/1930 [00:10<19:49,  1.61it/s, loss=1.39, v_num=393s]Epoch 0:   1% 16/1930 [00:10<19:49,  1.61it/s, loss=1.39, v_num=393s]Epoch 0:   1% 17/1930 [00:11<19:49,  1.61it/s, loss=1.39, v_num=393s]Epoch 0:   1% 17/1930 [00:11<19:49,  1.61it/s, loss=1.39, v_num=393s]Epoch 0:   1% 18/1930 [00:11<19:52,  1.60it/s, loss=1.39, v_num=393s]Epoch 0:   1% 18/1930 [00:11<19:52,  1.60it/s, loss=1.39, v_num=393s]Epoch 0:   1% 19/1930 [00:12<19:53,  1.60it/s, loss=1.39, v_num=393s]Epoch 0:   1% 19/1930 [00:12<19:53,  1.60it/s, loss=1.39, v_num=393s]Epoch 0:   1% 20/1930 [00:13<19:54,  1.60it/s, loss=1.39, v_num=393s]Epoch 0:   1% 20/1930 [00:13<19:54,  1.60it/s, loss=1.39, v_num=393s]Epoch 0:   1% 21/1930 [00:13<19:56,  1.60it/s, loss=1.39, v_num=393s]Epoch 0:   1% 21/1930 [00:13<19:56,  1.60it/s, loss=1.39, v_num=393s]Epoch 0:   1% 22/1930 [00:14<19:56,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   1% 22/1930 [00:14<19:56,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   1% 23/1930 [00:15<19:57,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   1% 23/1930 [00:15<19:57,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   1% 24/1930 [00:15<19:57,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   1% 24/1930 [00:15<19:57,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   1% 25/1930 [00:16<19:58,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   1% 25/1930 [00:16<19:58,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   1% 26/1930 [00:16<19:58,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   1% 26/1930 [00:17<19:58,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   1% 27/1930 [00:17<19:58,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   1% 27/1930 [00:17<19:58,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   1% 28/1930 [00:18<19:59,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   1% 28/1930 [00:18<19:59,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   2% 29/1930 [00:18<19:57,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   2% 29/1930 [00:18<19:57,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   2% 30/1930 [00:19<19:56,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   2% 30/1930 [00:19<19:56,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   2% 31/1930 [00:20<19:56,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   2% 31/1930 [00:20<19:56,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   2% 32/1930 [00:20<19:56,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   2% 32/1930 [00:20<19:56,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   2% 33/1930 [00:21<19:56,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   2% 33/1930 [00:21<19:56,  1.59it/s, loss=1.39, v_num=393s]Epoch 0:   2% 34/1930 [00:22<19:57,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 34/1930 [00:22<19:57,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 35/1930 [00:22<19:56,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 35/1930 [00:22<19:56,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 36/1930 [00:23<19:55,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 36/1930 [00:23<19:55,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 37/1930 [00:24<19:56,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 37/1930 [00:24<19:56,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 38/1930 [00:24<19:55,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 38/1930 [00:24<19:55,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 39/1930 [00:25<19:55,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 39/1930 [00:25<19:55,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 40/1930 [00:25<19:54,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 40/1930 [00:25<19:54,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 41/1930 [00:26<19:52,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 41/1930 [00:26<19:52,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 42/1930 [00:27<19:52,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 42/1930 [00:27<19:52,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 43/1930 [00:27<19:51,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 43/1930 [00:27<19:51,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 44/1930 [00:28<19:51,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 44/1930 [00:28<19:51,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 45/1930 [00:29<19:51,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 45/1930 [00:29<19:51,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 46/1930 [00:29<19:51,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 46/1930 [00:29<19:51,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 47/1930 [00:30<19:50,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 47/1930 [00:30<19:50,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 48/1930 [00:30<19:48,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   2% 48/1930 [00:30<19:48,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   3% 49/1930 [00:31<19:47,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   3% 49/1930 [00:31<19:47,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   3% 50/1930 [00:32<19:46,  1.58it/s, loss=1.39, v_num=393s]Epoch 0:   3% 50/1930 [00:32<19:46,  1.58it/s, loss=1.39, v_num=393s]Traceback (most recent call last):
  File "src/models/train_model.py", line 105, in <module>
    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)
  File "/home/kce/miniconda3/envs/snp-compression/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 553, in fit
    self._run(model)
  File "/home/kce/miniconda3/envs/snp-compression/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 918, in _run
    self._dispatch()
  File "/home/kce/miniconda3/envs/snp-compression/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 986, in _dispatch
    self.accelerator.start_training(self)
  File "/home/kce/miniconda3/envs/snp-compression/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 92, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/kce/miniconda3/envs/snp-compression/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 161, in start_training
    self._results = trainer.run_stage()
  File "/home/kce/miniconda3/envs/snp-compression/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 996, in run_stage
    return self._run_train()
  File "/home/kce/miniconda3/envs/snp-compression/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 1045, in _run_train
    self.fit_loop.run()
  File "/home/kce/miniconda3/envs/snp-compression/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 111, in run
    self.advance(*args, **kwargs)
  File "/home/kce/miniconda3/envs/snp-compression/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py", line 200, in advance
    epoch_output = self.epoch_loop.run(train_dataloader)
  File "/home/kce/miniconda3/envs/snp-compression/lib/python3.8/site-packages/pytorch_lightning/loops/base.py", line 111, in run
    self.advance(*args, **kwargs)
  File "/home/kce/miniconda3/envs/snp-compression/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 163, in advance
    self.trainer.logger_connector.update_train_step_metrics()
  File "/home/kce/miniconda3/envs/snp-compression/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py", line 217, in update_train_step_metrics
    self.log_metrics(self.metrics[MetricSource.LOG])
  File "/home/kce/miniconda3/envs/snp-compression/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py", line 96, in log_metrics
    scalar_metrics = metrics_to_scalars(metrics)
  File "/home/kce/miniconda3/envs/snp-compression/lib/python3.8/site-packages/pytorch_lightning/utilities/metrics.py", line 40, in metrics_to_scalars
    return apply_to_collection(metrics, torch.Tensor, to_item)
  File "/home/kce/miniconda3/envs/snp-compression/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py", line 104, in apply_to_collection
    v = apply_to_collection(
  File "/home/kce/miniconda3/envs/snp-compression/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py", line 96, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/home/kce/miniconda3/envs/snp-compression/lib/python3.8/site-packages/pytorch_lightning/utilities/metrics.py", line 35, in to_item
    raise MisconfigurationException(
pytorch_lightning.utilities.exceptions.MisconfigurationException: The metric `tensor([[ 7293.,     0.,     0.,     0.],
        [35122.,     0.,     0.,     0.],
        [97649.,     0.,     0.,     0.],
        [  288.,     0.,     0.,     0.]])` does not contain a single element, thus it cannot be converted to a scalar.
wandb: Waiting for W&B process to finish, PID 10840
wandb: Program failed with code 1.  Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.02MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.02MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.02MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.02MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.02MB uploaded (0.00MB deduped)wandb: / 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: - 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: \ 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: | 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: / 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: - 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: \ 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb: | 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /home/kce/github/snp-compression/wandb/run-20210827_122644-2rgw393s/logs/debug.log
wandb: Find internal logs for this run at: /home/kce/github/snp-compression/wandb/run-20210827_122644-2rgw393s/logs/debug-internal.log
wandb: Run summary:
wandb:                                        _runtime 45
wandb:                                      _timestamp 1630060049
wandb:                                           _step 0
wandb: Run history:
wandb:     _runtime ▁
wandb:   _timestamp ▁
wandb:        _step ▁
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced quiet-river-27: https://wandb.ai/kenevoldsen/snp-compression-src_models/runs/2rgw393s

