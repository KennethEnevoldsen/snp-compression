Fine-tunin SNPNet
wandb: Currently logged in as: danish_embeddings (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.17 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.1
wandb: Syncing run skilled-sun-1
wandb:  View project at https://wandb.ai/danish_embeddings/snp-classifiers-height
wandb:  View run at https://wandb.ai/danish_embeddings/snp-classifiers-height/runs/33yc2uv2
wandb: Run data is saved locally in /home/kce/NLPPred/snp-compression/clf_models/wandb/run-20220527_152530-33yc2uv2
wandb: Run `wandb offline` to turn off syncing.
/home/kce/miniconda3/envs/snpnet/lib/python3.9/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing is producing a large chunk. To accept the large
chunk and silence this warning, set the option
    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):
    ...     array[indexer]

To avoid creating the large chunks, set the option
    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):
    ...     array[indexer]
  return self.array[key]
/home/kce/miniconda3/envs/snpnet/lib/python3.9/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing is producing a large chunk. To accept the large
chunk and silence this warning, set the option
    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):
    ...     array[indexer]

To avoid creating the large chunks, set the option
    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):
    ...     array[indexer]
  return self.array[key]
Using 16bit native Automatic Mixed Precision (AMP)
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Set SLURM handle signals.

Loading Model
Loading Model
Loading Model
slurmstepd: error: *** JOB 61635 ON s10n02 CANCELLED AT 2022-05-27T15:28:20 ***
