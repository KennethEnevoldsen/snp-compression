Fine-tuning SNPNet
wandb: Currently logged in as: kenevoldsen. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.12.17
wandb: Run data is saved locally in /home/kce/NLPPred/snp-compression/clf_models/wandb/run-20220528_133149-26adrk02
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run avid-forest-84
wandb: ⭐️ View project at https://wandb.ai/kenevoldsen/snp-classifiers-height
wandb: 🚀 View run at https://wandb.ai/kenevoldsen/snp-classifiers-height/runs/26adrk02
/home/kce/miniconda3/envs/snpnet/lib/python3.9/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing is producing a large chunk. To accept the large
chunk and silence this warning, set the option
    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):
    ...     array[indexer]

To avoid creating the large chunks, set the option
    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):
    ...     array[indexer]
  return self.array[key]
/home/kce/miniconda3/envs/snpnet/lib/python3.9/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing is producing a large chunk. To accept the large
chunk and silence this warning, set the option
    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):
    ...     array[indexer]

To avoid creating the large chunks, set the option
    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):
    ...     array[indexer]
  return self.array[key]
Using 16bit native Automatic Mixed Precision (AMP)
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Set SLURM handle signals.

  | Name      | Type       | Params
-----------------------------------------
0 | encoders  | ModuleDict | 3.1 M 
1 | loss      | MSELoss    | 0     
2 | clf_layer | Linear     | 4.5 K 
-----------------------------------------
3.1 M     Trainable params
0         Non-trainable params
3.1 M     Total params
6.201     Total estimated model params size (MB)
Start setting up data loaders
Finished setting up data loaders
Loading model
Loading Model
Loading Model
Loading Model
On Device: cpu
Model loaded
device cpu
Setting up trainer
Finished setting up trainer
config.gpus: 1
trainer.gpus: 1
False
<class 'bool'>
Started model fitting
Validation sanity check: 0it [00:00, ?it/s]/home/kce/miniconda3/envs/snpnet/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 36 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Validation sanity check:   0% 0/2 [00:00<?, ?it/s]/home/kce/miniconda3/envs/snpnet/lib/python3.9/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
Validation sanity check:  50% 1/2 [00:02<00:02,  2.28s/it]Validation sanity check: 100% 2/2 [00:02<00:00,  1.01s/it]                                                          /home/kce/miniconda3/envs/snpnet/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 36 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Training: 0it [00:00, ?it/s]Training: 0it [00:00, ?it/s]Epoch 0: : 0it [00:00, ?it/s]Epoch 0: : 1it [00:02,  2.85s/it]Epoch 0: : 1it [00:02,  2.85s/it, loss=3.53]Epoch 0: : 2it [00:03,  1.75s/it, loss=3.53]Epoch 0: : 2it [00:03,  1.75s/it, loss=3.67]Epoch 0: : 3it [00:04,  1.38s/it, loss=3.67]Epoch 0: : 3it [00:04,  1.38s/it, loss=3.65]Epoch 0: : 4it [00:04,  1.19s/it, loss=3.65]Epoch 0: : 4it [00:04,  1.19s/it, loss=3.53]Epoch 0: : 5it [00:05,  1.08s/it, loss=3.53]Epoch 0: : 5it [00:05,  1.08s/it, loss=7.44]Epoch 0: : 6it [00:06,  1.01s/it, loss=7.44]Epoch 0: : 6it [00:06,  1.01s/it, loss=9.75]Epoch 0: : 7it [00:06,  1.04it/s, loss=9.75]Epoch 0: : 7it [00:06,  1.04it/s, loss=11.7]Epoch 0: : 8it [00:07,  1.09it/s, loss=11.7]Epoch 0: : 8it [00:07,  1.09it/s, loss=10.8]Epoch 0: : 9it [00:07,  1.13it/s, loss=10.8]Epoch 0: : 9it [00:07,  1.13it/s, loss=10.2]Epoch 0: : 10it [00:08,  1.16it/s, loss=10.2]Epoch 0: : 10it [00:08,  1.16it/s, loss=10.6]Epoch 0: : 11it [00:09,  1.19it/s, loss=10.6]Epoch 0: : 11it [00:09,  1.19it/s, loss=10.4]Epoch 0: : 12it [00:09,  1.21it/s, loss=10.4]Epoch 0: : 12it [00:09,  1.21it/s, loss=9.68]Epoch 0: : 13it [00:10,  1.23it/s, loss=9.68]Epoch 0: : 13it [00:10,  1.23it/s, loss=9.09]Epoch 0: : 14it [00:11,  1.24it/s, loss=9.09]Epoch 0: : 14it [00:11,  1.24it/s, loss=8.82]Epoch 0: : 15it [00:11,  1.26it/s, loss=8.82]Epoch 0: : 15it [00:11,  1.26it/s, loss=8.76]Epoch 0: : 16it [00:14,  1.08it/s, loss=8.76]Epoch 0: : 16it [00:14,  1.08it/s, loss=8.46]Epoch 0: : 17it [00:15,  1.10it/s, loss=8.46]Epoch 0: : 17it [00:15,  1.10it/s, loss=8.04]Epoch 0: : 18it [00:16,  1.12it/s, loss=8.04]Epoch 0: : 18it [00:16,  1.12it/s, loss=7.69]Epoch 0: : 19it [00:16,  1.13it/s, loss=7.69]Epoch 0: : 19it [00:16,  1.13it/s, loss=7.47]Epoch 0: : 20it [00:17,  1.15it/s, loss=7.47]Epoch 0: : 20it [00:17,  1.15it/s, loss=7.3] Epoch 0: : 21it [00:18,  1.16it/s, loss=7.3]Epoch 0: : 21it [00:18,  1.16it/s, loss=7.29]Epoch 0: : 22it [00:18,  1.18it/s, loss=7.29]Epoch 0: : 22it [00:18,  1.18it/s, loss=7.17]Epoch 0: : 23it [00:19,  1.19it/s, loss=7.17]Epoch 0: : 23it [00:19,  1.19it/s, loss=7.05]Epoch 0: : 24it [00:20,  1.20it/s, loss=7.05]Epoch 0: : 24it [00:20,  1.20it/s, loss=6.99]Epoch 0: : 25it [00:20,  1.21it/s, loss=6.99]Epoch 0: : 25it [00:20,  1.21it/s, loss=5.95]Epoch 0: : 26it [00:21,  1.22it/s, loss=5.95]Epoch 0: : 26it [00:21,  1.22it/s, loss=4.99]Epoch 0: : 27it [00:21,  1.23it/s, loss=4.99]Epoch 0: : 27it [00:21,  1.23it/s, loss=3.89]Epoch 0: : 28it [00:22,  1.24it/s, loss=3.89]Epoch 0: : 28it [00:22,  1.24it/s, loss=3.75]Epoch 0: : 29it [00:23,  1.25it/s, loss=3.75]Epoch 0: : 29it [00:23,  1.25it/s, loss=3.54]Epoch 0: : 30it [00:23,  1.25it/s, loss=3.54]Epoch 0: : 30it [00:23,  1.25it/s, loss=2.95]Epoch 0: : 31it [00:24,  1.26it/s, loss=2.95]Epoch 0: : 31it [00:24,  1.26it/s, loss=2.59]